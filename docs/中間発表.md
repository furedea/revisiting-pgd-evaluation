---
marp: true
theme: base
size: 4:3
paginate: true
---

<!-- _class: lead -->

# 中間発表

---

# 目次

**初期化手法とイタレーションに関する実験を用いた PGD によるロバスト性評価の再検証**

1. 背景
2. 前提知識
3. 卒論テーマ
4. 参考文献

---

# 敵対的学習の背景

**なぜロバスト性が必要なのか**

- コンピュータビジョンや音声認識におけるブレイクスルーにより**分類器 (モデル) の性能がセキュリティ上重要**になっている
	- 顔認証やマルウェア検出など
- しかし自然なデータで学習されたモデルの場合，入力に対して人間には区別できないわずかなノイズを含めることで**簡単に誤分類**することが知られる
	- 自動運転・医療などの失敗が許されない事例では致命的
- 敢えてモデルにそのようなノイズを含むデータを学習させることで上述のような誤分類をある程度防ぐ (**敵対的学習**) ことができると発見され，現在様々な防御手法が確立されている
→ **モデルを誤分類させる入力へのモデルの頑強性 (ロバスト性)** をきちんと評価する必要がある

---

# 前提知識 (モデルの定義)

**入力画像**を $\boldsymbol{x} \in \mathbb{R}^D$ ($D \in \mathbb{N}$ は入力次元数)，

**正解クラス**を $\boldsymbol{y} \in \mathbb{R}^K$ ($K \in \mathbb{N}$ はクラス数)，

**予測クラス**を $\hat{\boldsymbol{y}} \in \mathbb{R}^K$ ($K \in \mathbb{N}$ はクラス数)，

**モデル**(多クラス分類器) を

$$F(\boldsymbol{x}) = \hat{\boldsymbol{y}}$$

とする．

また，**損失関数**を $L(\boldsymbol{x},\boldsymbol{y})$ とする．

モデルや損失関数は**パラメータ**にも依存するが，本テーマではモデルの**評価**が主軸であり**パラメータ**は固定するため省略する．

---

# 前提知識 (敵対的学習の用語)

- **敵対的サンプル**：自然なデータに人間が知覚できない微小なノイズを加えた入力
- **(敵対的) 摂動**：モデルの推論を誤らせるため入力に加える微小なノイズ
- **敵対的攻撃**：敵対的サンプルによりモデルの推論を誤らせる
- **(敵対的) ロバスト性**：敵対的攻撃に対しモデルが正しい出力を維持する能力
- **敵対的学習 (訓練)**：敵対的サンプルを訓練データに組み込んで学習させモデルの敵対的ロバスト性を向上させる防御手法

---

# 前提知識 (敵対的学習の式定義)

次ページ以降紹介する**敵対的攻撃手法**(PGD, DeepFool) は，定められた処理を反復しながら

**「モデルを誤分類させるような敵対的摂動を作る」**

ことが目的．

$t(t \in \mathbb{N})$ 回目の反復の際の敵対的サンプルを $\boldsymbol{x}^t_{adv}$，
敵対的摂動を $\boldsymbol{\delta}_t$ とすると，

$$\boldsymbol{x}^t_{adv} = \boldsymbol{x} + \boldsymbol{\delta}_t$$

---

# 前提知識 (PGD の概要)

**PGD(射影勾配降下法)**：(制約範囲内で) 入力に関する**損失の勾配方向**に摂動を反復して更新する攻撃手法

敵対的摂動は小さすぎると強力な敵対的サンプルが作れず，大きすぎると画像の持つ意味が変わってしまうため範囲を定める．

敵対的摂動の $\ell_\infty$ 制約下の範囲を

$$\|\boldsymbol{\delta}_t\|_{\infty} \leq \varepsilon \quad (\varepsilon > 0)$$

とし，制約を満たす摂動の集合を $\mathcal{S}$ とする．

入力に関する損失の勾配方向に摂動を更新する際に摂動が範囲外にあった場合，**範囲内の最も近い点に射影**する．

> \[1]: 稲岡 陽向，研究室内スライド「L1 正則化ニューラルネットワークの特徴量混合による敵対的堅牢性」，2025 年 5 月 27 日

---

# 前提知識 (PGD の更新式)

PGD の更新式は

$$\boldsymbol{x}^{t + 1}_{adv} = \Pi_{\boldsymbol{x} + \mathcal{S}} (\boldsymbol{x}^t_{adv} + \alpha\ \text{sgn}(\nabla_\boldsymbol{x} L(\boldsymbol{x}^t_{adv}, \boldsymbol{y}))) \quad ( 0 < \alpha < \varepsilon)$$

- 射影演算子 $\Pi_{\boldsymbol{x} + \mathcal{S}}(・)$：括弧内のベクトルが敵対的サンプルの制約範囲 $\boldsymbol{x}+\mathcal{S}$ を超えた場合範囲内の最も近い点に射影

定められた最大反復数に達すると反復を終了．

**ランダム初期化 PGD**

また，PGD が入力点から開始するのに対し，**ランダム初期化 PGD** は制約範囲内のランダムな点から開始する．

---

# 前提知識 (DeepFool の概要)

**DeepFool**：現在の点から**最も近いクラス境界へ最短で向かう方向**に摂動を反復して更新する攻撃手法

ニューラルネットを現在の点の近くで**局所的に線形**だとみなす．
分類器が線形ならば，各クラスのクラス境界は**直線や (超) 平面** で表されるため，各クラス境界までの**最短摂動**(大きさが現在の点からクラス境界までの距離で，向きが境界に垂直なベクトル) を導出できる．

現在の点から (予測クラス除く)**最も境界が近いクラス**を選び，そのクラス境界への最短方向の摂動を加える．この手順を**反復**することで，実際には非線形であるニューラルネットで誤分類を起こす敵対的サンプルを作る．

---

# 前提知識 (DeepFool①)

$t(t \in \mathbb{N})$ 回目の反復の際の点を $\boldsymbol{x}_t$，
ロジット (活性化関数適用前の出力) を

$$Z(\boldsymbol{x}) = (Z_1(\boldsymbol{x}), \dots, Z_K(\boldsymbol{x}))$$

とすると，予測クラス (のインデックス) を以下のように表せる．

$$y_t = \arg\max_k Z_k(\boldsymbol{x}_t)$$

各クラス $k \neq y_t$ について，ロジットのスコアの差を

$$g_k(\boldsymbol{x}) = Z_k(\boldsymbol{x}) - Z_{y_t}(\boldsymbol{x}) \quad (g_k(\boldsymbol{x}) \in \mathbb{R})$$

とおき，点 $\boldsymbol{x}_t$ 近傍の一次のテイラー展開：

$$g_k(\boldsymbol{x}) \approx g_k(\boldsymbol{x}_t) + \nabla g_k(\boldsymbol{x}_t)^\top (\boldsymbol{x} - \boldsymbol{x}_t)$$

---

# 前提知識 (DeepFool②)

**線形近似**されたモデルに対し，クラス境界までの最短摂動は

$$\boldsymbol{r}_{t, k} = - \frac{g_k(\boldsymbol{x}_t)} {\|\nabla g_k(\boldsymbol{x}_t)\|_2^2} \nabla g_k(\boldsymbol{x}_t),\ \ \|\boldsymbol{r}_{t, k}\|_2 = \frac{|g_k(\boldsymbol{x}_t)|} {\|\nabla g_k(\boldsymbol{x}_t)\|_2}$$

と導出でき，現在の点 $\boldsymbol{x}_t$ から最もクラス境界が近いクラス $k^\ast$ は

$$k^\ast_t = \arg\min_{k \ne y_t} \|\boldsymbol{r}_{t, k}\|_2$$

と表せる．線形近似によりクラス境界のズレがあり，確実に境界を超えるため，更新式は以下：

$$\boldsymbol{x}_{i+1} = \boldsymbol{x}_i + (1+\eta)\boldsymbol{r}_{i, k} \quad (\eta > 0)$$

定められた最大反復数に達すると反復を終了．

---

# 卒論テーマに関わる論文

**着目した論文**

PGD をモデルの敵対的ロバスト性を学習・評価する攻撃手法として提案した論文 $^{[1]}$

**着目した実験**

非凸な関数の最大化を取り扱う PGD が安定してある程度同じ局所最大値を見つけられるか，ひいてはロバスト性評価のための攻撃手法として適切なのかを，**イタレーション毎の損失の収束の仕方**を可視化することで確かめる実験

> \[1]: Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, “Towards Deep Learning Models Resistant to Adversarial Attacks,” International Conference on Learning Representations (ICLR), 2018.

---

# 卒論テーマに関わる実験①

Figure 1：MNIST 訓練済みモデルに **ランダム初期化 PGD 攻撃**を 20 回行った場合のイタレーション毎の損失の推移

- 横軸：1 回の PGD のイタレーション
- 縦軸：クロスエントロピー損失

> \[1]: Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, “Towards Deep Learning Models Resistant to Adversarial Attacks,” International Conference on Learning Representations (ICLR), 2018.

---

# 卒論テーマに関わる実験②

Figure 2：CIFAR-10 訓練済みモデルに**ランダム初期化 PGD 攻撃**を 20 回行った場合のイタレーション毎の損失の推移 (Figure 1 と同様の見方)

> \[1]: Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, “Towards Deep Learning Models Resistant to Adversarial Attacks,” International Conference on Learning Representations (ICLR), 2018.

---

# 卒論テーマ

**初期化手法とイタレーションに関する実験を用いた PGD によるロバスト性評価の再検証**

- PGD によるロバスト性の評価はどのくらい信頼できるものなのか？
- どの初期化手法でどのくらいのイタレーションを回せば損失がプラトー付近になるのか？
	- より少ないイタレーションで損失をプラトー付近に到達させられるかもしれない**DeepFool の境界情報を用いた初期化**を提案 (次ページ)
	- 各場合のイタレーション毎の損失，損失の収束までのイタレーションの平均や分布，モデルの精度の推移を調査

---

# 初期化手法の提案

**DeepFool の境界情報を用いた初期化手法**

- DeepFool で導出した敵対的サンプルから PGD を開始
	- DeepFool：現在の点から**最も近いクラス境界へ最短で向かう方向**に摂動を更新
	- PGD：入力に関する**損失の勾配方向**に摂動を更新
- DeepFool にも計算コストがかかるため，DeepFool の計算量とその後の PGD の計算量の合計が他の初期化手法の計算量と等しくなるように擦り合わせる
- DeepFool は $\ell_2$ **最短摂動**を狙う攻撃のため PGD の $\ell_\infty$ 制約範囲に射影して初期点とする

---

# 卒論テーマの実験対象

- モデル：CIFAR-10・MNIST の訓練済み自然学習モデルと PGD ロバストモデル
- 初期化手法：
	- 入力点
	- $\ell_\infty$ 制約範囲内のランダム初期化
	- DeepFool の境界情報を用いた初期化
- 調査対象：
	- 各モデル・初期化に対するイタレーション毎の**損失の推移**
	- 損失の収束までのイタレーションの平均や分布
		- 「入力点から始まる PGD は平均 8 イタレーションでプラトー，ランダム初期化 PGD は平均 4 イタレーション」などの具体的な数値の導出
	- モデルの精度の推移

---

# 参考文献

1. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, “Towards Deep Learning Models Resistant to Adversarial Attacks,” International Conference on Learning Representations (ICLR), 2018.
2. Moosavi-Dezfooli, S.-M., Fawzi, A., & Frossard, P. (2016). _DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks_. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2016), 2574-2582. IEEE.

PGD の l_infty の説明 (図の正方形になるっていう)
教授がーと言わない
テーマの動機をしっかりする (deepfool 適用の動機)
