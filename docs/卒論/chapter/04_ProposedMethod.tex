\chapter{提案手法}
\label{chapter:proposed}
本章では，PGD攻撃の初期化にDeepFoolの境界情報を活用する手法を提案する．この手法の目的は，PGD攻撃の誤分類達成反復数を少なくできる可能性を検証することである．

\section{DeepFool初期化の動機}
\label{sec:motivation}
本節では，DeepFoolの境界情報をPGD攻撃の初期化に活用する動機を述べる．

\subsection{PGD攻撃の初期化}
PGD攻撃は\ref{sec:pgd}節で述べたように，制約範囲内で損失を最大化する方向に反復的に摂動を更新する手法である．ランダム初期化PGDでは，$\ell_\infty$制約範囲内の一様乱数点から攻撃を開始する．この初期点は損失関数の形状やラベル境界の位置とは無関係にランダムに選ばれるため，必ずしも効率的な探索開始点とは限らない．

一方，DeepFoolは\ref{sec:deepfool}節で述べたように，最も近いラベル境界へ向かう方向に摂動を更新することで敵対的サンプルを生成する．DeepFoolが見つけた敵対的サンプルはラベル境界付近に位置しており，この境界情報をPGD攻撃の初期化に活用することで，より効率的な探索が可能になると考えられる．

\section{DeepFool初期化の手順}
本節では，DeepFool初期化の具体的な手順を説明する．

\subsection{DeepFoolによる敵対的サンプルの生成}
まず，入力画像$x$に対してDeepFoolを適用し，敵対的サンプル$x_{\text{df}}$を生成する．DeepFoolの更新式は式(\ref{eq:deepfool_update})で示した通りであり，パラメータとして最大反復数$T_{\text{df}}$とオーバーシュート係数$\eta$を設定する．本研究では$T_{\text{df}} = 50$，$\eta = 0.02$を用いる．

\subsection{$\ell_\infty$制約への射影}
\label{sec:projection}
DeepFoolは$\ell_2$ノルム最小の摂動を求める手法であるため，得られた敵対的サンプル$x_{\text{df}}$はPGD攻撃の$\ell_\infty$制約を満たさない場合がある．そのため，$x_{\text{df}}$を$\ell_\infty$制約範囲に射影してPGDの初期点$x_{\text{init}}$を得る必要がある．

本研究では，成分毎クリッピングによる射影を用いる．$x_{\text{df}}$の各成分を$[x - \varepsilon, x + \varepsilon]$の範囲に収めることで，式(\ref{eq:clip})のようにPGDの初期点$x_{\text{init}}$を得る．
\begin{align}
    x_{\text{init}} = \max(x - \varepsilon, \min(x_{\text{df}}, x + \varepsilon))
    \label{eq:clip}
\end{align}
ここで$\max$および$\min$は成分毎に適用される．

\subsection{PGD攻撃の実行}
射影後の初期点$x_{\text{init}}$から通常のPGD攻撃を開始する．更新式は式(\ref{eq:pgd})と同様であり，$x^0 = x_{\text{init}}$として反復を行う．

DeepFool初期化によるPGD攻撃の全体の手順をAlgorithm \ref{alg:deepfool_init}に示す．

\begin{algorithm}[H]
\caption{DeepFool初期化PGD}
\label{alg:deepfool_init}
\begin{algorithmic}[1]
\REQUIRE 入力画像$x$，正解ラベル$y$，摂動制約$\varepsilon$，ステップサイズ$\alpha$，PGD反復数$T$，DeepFool反復数$T_{\text{df}}$，オーバーシュート係数$\eta$
\STATE $x_{\text{df}}^0 \leftarrow x$，$t \leftarrow 0$
\WHILE{$k(x_{\text{df}}^t) = k(x)$ \AND $t < T_{\text{df}}$}
    \FOR{$k \neq k(x_{\text{df}}^t)$}
        \STATE $r^t_k \leftarrow -\frac{g_k(x_{\text{df}}^t)}{\|\nabla g_k(x_{\text{df}}^t)\|_2^2} \nabla g_k(x_{\text{df}}^t)$
    \ENDFOR
    \STATE $k^* \leftarrow \arg\min_{k \neq k(x_{\text{df}}^t)} \|r^t_k\|_2$
    \STATE $x_{\text{df}}^{t+1} \leftarrow x_{\text{df}}^t + (1 + \eta) r^t_{k^*}$
    \STATE $t \leftarrow t + 1$
\ENDWHILE
\STATE $x^0 \leftarrow \max(x - \varepsilon, \min(x_{\text{df}}^t, x + \varepsilon))$
\FOR{$t' = 0$ to $T - 1$}
    \STATE $x^{t'+1} \leftarrow \Pi_{x + \mathcal{S}} \left( x^{t'} + \alpha \cdot \text{sign}\left( \nabla_{x} L(x^{t'}, y) \right) \right)$
\ENDFOR
\RETURN $x^T$
\end{algorithmic}
\end{algorithm}

\section{Multi-DeepFool初期化}
\label{sec:multi_deepfool}

\subsection{動機}
DeepFool初期化には以下の2つの課題がある：
\begin{itemize}
    \item DeepFoolは各反復において正解ラベルを除く全ラベルへの最短摂動を計算するが，最終的には最も近いラベルへの摂動のみを採用する．他のラベル境界への情報は破棄される．
    \item DeepFoolは決定的なアルゴリズムであるため，1つの入力に対して1つの敵対的サンプルしか生成できない．ランダム初期化のように複数の初期点を試すことができない．
\end{itemize}

これらの課題を解決するため，各ターゲットラベルに対して個別にDeepFoolを実行し，複数の初期点を生成するMulti-DeepFool初期化を提案する．最も近いラベル以外のラベル境界がPGD攻撃にとってより有利な初期点を与える可能性があり，複数の初期点を試すことでより高い損失を達成できる可能性がある．

\subsection{複数ターゲットへのDeepFool}
予測ラベル$y$以外の各ラベル$k \neq y$について，ラベル$k$を目標としたDeepFoolを実行する．ラベル$k$を目標とするDeepFoolでは，式(\ref{eq:deepfool_rk})において$k^* = k$と固定して更新を行う．これにより，各ラベル$k$に対して敵対的サンプル$x^k_{\text{df}}$が得られる．$K$クラス分類問題においては，$(K-1)$個の敵対的サンプルが生成される．

\subsection{複数初期点からのPGD攻撃}
各ターゲットラベルに対するDeepFool結果を$\ell_\infty$制約範囲に射影し，それぞれを初期点としてPGD攻撃を実行する．すなわち，$(K-1)$個の初期点$x^k_{\text{init}}$（$k \neq y$）それぞれから独立にPGD攻撃を行う．

ランダム初期化との違いとして，Multi-DeepFool初期化では各初期点が決定的に生成される．そのため，同一サンプルに対しては常に同じ$(K-1)$個の初期点が得られる．

Multi-DeepFool初期化によるPGD攻撃の全体の手順をAlgorithm \ref{alg:multi_deepfool_init}に示す．

\begin{algorithm}[H]
\caption{Multi-DeepFool初期化PGD}
\label{alg:multi_deepfool_init}
\begin{algorithmic}[1]
\REQUIRE 入力画像$x$，正解ラベル$y$，ラベル数$K$，摂動制約$\varepsilon$，ステップサイズ$\alpha$，PGD反復数$T$，DeepFool反復数$T_{\text{df}}$，オーバーシュート係数$\eta$
\ENSURE 敵対的サンプル集合$\{x^{k,T}\}_{k \neq y}$
\FOR{$k \neq y$}
    \STATE $x_{\text{df}}^{k,0} \leftarrow x$，$t \leftarrow 0$
    \WHILE{$k(x_{\text{df}}^{k,t}) = k(x)$ \AND $t < T_{\text{df}}$}
        \STATE $r^t_k \leftarrow -\frac{g_k(x_{\text{df}}^{k,t})}{\|\nabla g_k(x_{\text{df}}^{k,t})\|_2^2} \nabla g_k(x_{\text{df}}^{k,t})$
        \STATE $x_{\text{df}}^{k,t+1} \leftarrow x_{\text{df}}^{k,t} + (1 + \eta) r^t_k$
        \STATE $t \leftarrow t + 1$
    \ENDWHILE
    \STATE $x^{k,0} \leftarrow \max(x - \varepsilon, \min(x_{\text{df}}^{k,t}, x + \varepsilon))$
    \FOR{$t' = 0$ to $T - 1$}
        \STATE $x^{k,t'+1} \leftarrow \Pi_{x + \mathcal{S}} \left( x^{k,t'} + \alpha \cdot \text{sign}\left( \nabla_{x} L(x^{k,t'}, y) \right) \right)$
    \ENDFOR
\ENDFOR
\RETURN $\{x^{k,T}\}_{k \neq y}$
\end{algorithmic}
\end{algorithm}

なお，提案手法の計算コストに関する分析は付録\ref{appendix:computational_cost}に示す．
