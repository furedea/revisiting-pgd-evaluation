\chapter{序論}
近年，深層学習は急速に発展し，画像分類や音声認識，自然言語処理など様々な分野で高い性能を達成している．特に画像分類においては，人間と同等あるいはそれ以上の精度を持つモデルが実現され，自動運転や医療診断，顔認証システムなどの実用的なアプリケーションに広く応用されている．

しかし，深層学習モデルは，入力画像に対し人間には知覚できないほど微小なノイズを加える敵対的攻撃(adversarial attack)を行うことで，モデルを容易に誤分類させられることが知られている．このようなノイズは敵対的摂動(adversarial perturbations)と呼ばれ，敵対的摂動が加えられた画像は敵対的サンプル(adversarial samples)と呼ばれる．敵対的サンプルは，元の画像と視覚的にはほぼ区別がつかないにもかかわらず，モデルの予測結果を大きく変化させてしまう．

敵対的サンプルの存在は，深層学習システムのセキュリティ上の重大な懸念事項となっている．例えば，自動運転車において道路標識の認識が攻撃された場合や，顔認証システムにおいてなりすましが行われた場合など，実社会においても深刻な影響をもたらす可能性がある．このため，モデルの敵対的ロバスト性(adversarial robustness)，すなわち敵対的サンプルに対しモデルが正しい出力を維持する能力を正確に評価し，向上させることが重要な課題となっている．

敵対的ロバスト性の評価では，強力な敵対的サンプルを生成しモデルに入力することでその精度を測る．このような敵対的サンプルを生成するには，人間が知覚しにくい範囲で入力を変化させ，モデルが誤分類しやすい敵対的摂動を探索する必要がある．Madry ら \cite{PGD} はこの探索を，制約付きで損失(誤分類のしやすさ)を最大化する問題と捉え，射影勾配降下法（Projected Gradient Descent; PGD)を強力な敵対的攻撃手法として用いることを提案した．これをPGD攻撃(Projected Gradient Descent attack)という．

しかし，PGD攻撃は近似的な解法であり，常に大域的最適解，すなわち最も強力な敵対的サンプルを見つけられる保証はない．そこでMadryらは，PGD攻撃が安定して同程度の局所最適解を見つけられるかどうかを，異なる初期点から開始した複数のPGD攻撃の損失の推移を可視化することで検証した\cite{PGD}．この実験では，ランダムな初期点から開始しても十分な反復を経ることで損失曲線が同程度の値に収束することが示され，PGD攻撃がロバスト性評価に適した攻撃手法であることが示唆された．ただしこの検証は，どの段階で誤分類したのか，どこで損失が局所最大解に達したのかといった，PGD攻撃における最適な反復回数についての議論や，異なる初期化手法の場合については触れられていない．

本研究では，PGD攻撃によるロバスト性評価の信頼性について，反復回数と初期化手法の観点から再検証を行う．具体的には，まずMadryらの論文で報告された実験の再現を試み，その結果を比較検討する．また，設定を一部変更した異なるモデルも交え，誤分類や局所最大解への到達が起こる反復回数についての検証を行う．さらに，PGD攻撃の収束を早める可能性を持つ新たな初期化手法として，Deepfool\cite{DeepFool}の境界情報を用いた初期化を提案し，一般的な初期化手法を含めその効果を検証する．

本論文の構成は以下の通りである．第2章では本研究で扱うデータセット及び関連する攻撃手法について説明する．第3章では反復回数の観点からPGD攻撃の収束性を検討し，本研究で提案するDeepfoolを用いた初期化手法について詳述する．第4章では提案手法を含む複数の実験設定での評価実験について述べる．第5章で本研究の結論と今後の展望を述べる．
