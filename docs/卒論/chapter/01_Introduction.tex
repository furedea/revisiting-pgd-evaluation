% !TeX root = ../../main.tex
\chapter{序論}
近年，深層学習は急速に発展し，画像分類や音声認識，自然言語処理など様々な分野で高い性能を達成している．特に画像分類においては，人間と同等あるいはそれ以上の精度を持つモデルが実現され，自動運転や医療診断，顔認証システムなどの実用的なアプリケーションに広く応用されている．

しかし，深層学習モデルは入力画像に対し人間には知覚できないほど微小なノイズを加えることで，モデルを容易に誤分類させられることが知られている\cite{PGD}．このようなノイズは敵対的摂動(adversarial perturbations)と呼ばれ，敵対的摂動が加えられた画像は敵対的サンプル(adversarial samples)と呼ばれる．敵対的サンプルは，元の画像と視覚的にはほぼ区別がつかないにもかかわらず，モデルの予測結果を大きく変化させてしまう．

敵対的サンプルの存在は，深層学習システムのセキュリティ上の重大な懸念事項となっている．例えば，自動運転車において道路標識の認識が攻撃された場合や，顔認証システムにおいてなりすましが行われた場合など，実社会においても深刻な影響をもたらす可能性がある．このため，モデルの敵対的ロバスト性(adversarial robustness)，すなわち敵対的サンプルに対しモデルが正しい出力を維持する能力を正確に評価し，向上させることが重要な課題となっている．

敵対的ロバスト性を評価するための代表的な攻撃手法として，PGD攻撃(Projected Gradient Descent attack)がある\cite{PGD}．PGD攻撃は，モデルの損失関数を入力に関して最大化する方向に摂動を反復的に更新することで敵対的サンプルを生成する手法である．PGD攻撃は理論的にも実用的にも強力な攻撃手法として広く認知されており，モデルのロバスト性評価の標準的な手法として用いられている．

PGD攻撃は制約付き非凸最適化問題を解いているため，常に大域的最適解（最も強力な敵対的サンプル）を見つけられる保証はない．Madryら\cite{PGD}は，PGD攻撃が安定して同程度の局所最大値を見つけられるかどうかを，異なる初期点から開始した複数のPGD攻撃のイタレーション毎の損失の推移を可視化することで検証した．この実験では，ランダムな初期点から開始しても損失曲線が同程度のプラトー（平坦な領域）に収束することが示され，PGD攻撃がロバスト性評価に適した攻撃手法であることが示唆された．

本研究では，PGD攻撃によるロバスト性評価の信頼性について，初期化手法とイタレーション数の観点から再検証を行う．具体的には，まずMadryらの論文で報告された実験の再現を試み，その結果を比較検討する．さらに，PGD攻撃の収束を早める可能性を持つ新たな初期化手法として，DeepFool\cite{DeepFool}の境界情報を用いた初期化を提案し，その効果を検証する．

本論文の構成は以下の通りである．第2章では本研究で扱うデータセット及び関連する攻撃手法について説明する．第3章では本研究で提案するDeepFoolを用いた初期化手法について詳述する．第4章では提案手法を含む複数の実験設定での評価実験について述べる．第5章で本研究の結論と今後の展望を述べる．
