\chapter{序論}
\label{chapter:introduction}
近年，深層学習は急速に発展し，画像分類や音声認識，自然言語処理など様々な分野で高い性能を達成している．特に画像分類においては，人間と同等あるいはそれ以上の精度を持つモデルが実現され，自動運転や医療診断，顔認証システムなどの実用的なアプリケーションに広く応用されている．

しかし，深層学習モデルは，入力画像に対し人間には知覚できないほど微小なノイズを加える敵対的攻撃(adversarial attack)を行うことで，モデルを容易に誤分類させられることが知られている．このようなノイズは敵対的摂動(adversarial perturbations)と呼ばれ，敵対的摂動が加えられた画像は敵対的サンプル(adversarial samples)と呼ばれる．敵対的サンプルは，元の画像と視覚的にはほぼ区別がつかないにもかかわらず，モデルの予測結果を大きく変化させてしまう．

敵対的サンプルの存在は，深層学習システムのセキュリティ上の重大な懸念事項となっている．例えば，自動運転車において道路標識の認識が攻撃された場合や，顔認証システムにおいてなりすましが行われた場合など，実社会においても深刻な影響をもたらす可能性がある．このため，モデルの敵対的ロバスト性(adversarial robustness)，すなわち敵対的サンプルに対しモデルが正しい出力を維持する能力を正確に評価し，向上させることが重要な課題となっている．

敵対的ロバスト性の評価では，強力な敵対的サンプルを生成しモデルに入力することでその精度を測る．このような敵対的サンプルを生成するには，人間が知覚しにくい範囲で入力を変化させ，モデルが誤分類しやすい敵対的摂動を探索する必要がある．Madry ら \cite{PGD} はこの探索を，制約付きで損失(誤分類のしやすさ)を最大化する問題と捉え，射影勾配降下法（Projected Gradient Descent; PGD)を強力な敵対的攻撃手法として用いることを提案した．これをPGD攻撃(Projected Gradient Descent attack)という．

しかし，PGD攻撃は近似的な解法であり，常に強力な敵対的サンプルを安定して見つけられる保証はない．そこでMadryらは，PGD攻撃が安定して同程度の局所最大解を見つけられるかどうかを，異なる初期点から開始した複数のPGD攻撃の損失の推移を可視化することで検証した\cite{PGD}．この実験では，ランダムな初期点から開始しても十分な反復を経ることで損失曲線が同程度の値に収束することが示され，PGD攻撃がロバスト性評価に適した攻撃手法であることが示唆された．ただし，その安定して見つけられる強力な敵対的サンプルで実際に誤分類が達成されるかどうかや，その誤分類が起こる速さ（以下，誤分類速度と呼ぶ）がデータセットやモデル，初期化手法によりどのように異なるかについては触れられていない．

本研究では，Madryらの検証を誤分類の観点から拡張し，PGD攻撃の誤分類特性を定量的に評価する．具体的には，まずMadryらの論文で報告された実験の再現を試み，損失の収束に加え誤分類がいつ達成されるかを観察する．また，データセットやモデルによる誤分類速度の違いを評価する．さらに，PGD攻撃の誤分類を早める可能性を持つ新たな初期化手法として，DeepFool\cite{DeepFool}の境界情報を用いた初期化を提案し，その効果を検証する．

本論文の構成は以下の通りである．第2章では本研究で用いる敵対的攻撃手法について説明する．第3章ではPGD攻撃の誤分類を定量的に評価するための方法について述べる．第4章では本研究で提案するDeepFoolを用いた初期化手法について詳述する．第5章では実験の目的と対象を述べた上で，提案手法を含む複数の実験設定での評価実験とその結果について述べる．第6章で本研究の結論と今後の展望を述べる．
