\chapter{関連研究}

本章では，本研究で用いる敵対的攻撃手法と，本研究の着眼点となるMadryらの実験について説明する．

\section{PGD攻撃}

\label{sec:pgd}

敵対的サンプル$x_{\text{adv}}$は，入力画像$x$に敵対的摂動$\delta$を加えたものであり，式(\ref{eq:adv_sample})で表される．
\begin{align}
    x_{\text{adv}} = x + \delta
    \label{eq:adv_sample}
\end{align}

Madryらは，制約付き最適化問題を解くための標準的な手法であった射影勾配降下法（Projected Gradient Descent; PGD）をこの敵対的サンプルの生成に適用し，PGD攻撃として定式化した\cite{PGD}．

\subsection{問題設定}

損失関数$L(x, y)$は，モデルの出力$f(x)$と正解ラベル$y$の間の損失を表す．PGD攻撃は，敵対的摂動$\delta$の$\ell_\infty$ノルムが$\varepsilon$以下という制約のもとでこの損失を最大化するような，式(\ref{eq:pgd_objective})で定義される制約付き最適化問題を解く．

\begin{align}
    \max_{\delta} L(x + \delta, y) \quad \text{s.t.} \quad \|\delta\|_\infty \leq \varepsilon
    \label{eq:pgd_objective}
\end{align}

ここで$\varepsilon > 0$は摂動の強さを制御するパラメータであり，$\varepsilon$が大きいほどモデルを誤分類させる力が強くなる．ただし$\varepsilon$が大きすぎると，入力画像と別物と感じられる敵対的サンプルが作られる恐れがあり，同じ入力の小さな改変でモデルが誤分類するかどうかという議論が意味を成さなくなるため注意が必要である．

\subsection{更新式}

PGD攻撃では，以下に示す操作を$T$回反復する．$t$回目の反復における画像$x^t$を分類器$f$に入力した場合の損失$L$を計算し，その符号成分のみを取り出す．そして画像$x^t$をその符号成分の方向に学習率$\alpha$の分だけ変化させて$(t+1)$回目の反復における画像$x^{t+1}$を作成する．以上の操作を以下の式(\ref{eq:pgd})で表す．

\begin{align}
    x^{t+1} = \Pi_{x + \mathcal{S}} \left( x^t + \alpha \cdot \text{sign}\left( \nabla_{x} L(x^t, y) \right) \right) \quad (t = 0, 1, \ldots, T-1)
    \label{eq:pgd}
\end{align}

ここで$\text{sign}(\cdot)$は各成分の符号を取る関数であり，$\Pi_{x + \mathcal{S}}(\cdot)$は射影演算子である．$D$を入力画像の次元数として，$\mathcal{S} = \{\delta \in \mathbb{R}^D : \|\delta\|_\infty \leq \varepsilon\}$は許容される摂動の集合であり，更新後の点が制約範囲$x + \mathcal{S}$を超えた場合，範囲内の最も近い点に射影される．

\subsection{ランダム初期化}
PGD攻撃には初期点$x^0$の設定が必要である．入力点$x$から開始する方法もあるが，ランダム初期化PGDでは，式(\ref{eq:random_init})に示すように，入力画像$x$にランダムな摂動を加えた点から開始する．

\begin{align}
    x^0 = x + u, \quad u \sim \text{Uniform}(-\varepsilon, \varepsilon)^D
    \label{eq:random_init}
\end{align}

ここで$D$は入力画像の次元数である．この初期点から式(\ref{eq:pgd})の更新を$T$回繰り返し，最終的に得られる$x^T$が敵対的サンプルとなる．

ランダム初期化により初期点が異なると到達する局所最適解も異なる可能性があるため，一つの入力画像に対して複数の異なる初期点から攻撃を行い，最も損失が大きくなった敵対的サンプルを採用することが一般的である．この複数回の試行をリスタートと呼ぶ．

ランダム初期化PGD攻撃の全体の手順をAlgorithm \ref{alg:pgd}に示す．

\begin{algorithm}[htbp]
\caption{ランダム初期化PGD攻撃}
\label{alg:pgd}
\begin{algorithmic}[1]
\REQUIRE 入力画像$x$，正解ラベル$y$，摂動制約$\varepsilon$，ステップサイズ$\alpha$，反復回数$T$
\ENSURE 敵対的サンプル$x^T$
\STATE $u \sim \text{Uniform}(-\varepsilon, \varepsilon)^D$
\STATE $x^0 \leftarrow x + u$
\FOR{$t = 0$ to $T - 1$}
    \STATE $x^{t+1} \leftarrow \Pi_{x + \mathcal{S}} \left( x^t + \alpha \cdot \text{sign}\left( \nabla_{x} L(x^t, y) \right) \right)$
\ENDFOR
\RETURN $x^{T}$
\end{algorithmic}
\end{algorithm}

\section{DeepFool}
\label{sec:deepfool}

DeepFool\cite{DeepFool}はMoosavi-Dezfooliらによって提案された敵対的攻撃手法である．前節で述べたPGD攻撃が「摂動の$\ell_\infty$ノルムを$\varepsilon$以下に制約した上で損失を最大化する」という問題を解くのに対し，DeepFoolは「モデルを誤分類させるために必要な$\ell_2$ノルム最小の摂動を求める」という問題を解く．

\subsection{問題設定}

DeepFoolが解く問題は，式(\ref{eq:deepfool_objective})で定義される．

\begin{align}
    \min_{\delta} \|\delta\|_2 \quad \text{s.t.} \quad k(x + \delta) \neq k(x)
    \label{eq:deepfool_objective}
\end{align}

ここで$k(x)$はモデルの予測ラベルを表す．この問題は，予測ラベルを変化させる最小の摂動を求めることを意味する．一般の非線形分類器に対してこの問題を直接解くことは困難であるため，DeepFoolでは分類器を局所的に線形近似し，反復的に摂動を更新する手法を採用している．

\subsection{基本原理}

DeepFoolの基本原理は，ニューラルネットワークを現在の点の近傍で局所的に線形近似し，最も近いラベル境界へ向かう方向に摂動を更新するというものである．分類器が線形であれば，各ラベルの決定境界は超平面で表されるため，点から超平面までの最短距離を解析的に導出できる．

まず，2クラス分類の場合を考える．線形分類器$f(x) = w^\top x + b$において，決定境界は$f(x) = 0$という超平面である．分類を変化させるには決定境界を越える必要があり，$\ell_2$ノルム最小でこれを達成する摂動を最短摂動と定義する．最短摂動は超平面に垂直な方向（すなわち$w$の方向）に，点と超平面の距離だけ移動することで得られる．点$x_0$における最短摂動$r^*$は式(\ref{eq:deepfool_binary_r})で与えられる．
\begin{align}
    r^* = -\frac{f(x_0)}{\|w\|_2^2} w
    \label{eq:deepfool_binary_r}
\end{align}

DeepFoolはこのアイデアを多クラス分類に拡張し，さらに非線形分類器に対しては局所的な線形近似を用いて反復的に適用する．

\subsection{線形近似とラベル境界}

$t$回目の反復における点を$x^t$，モデルの正規化前の出力を$Z(x) = (Z_1(x), \ldots, Z_K(x))$とする．ここで$K$はラベル数である．現在の予測ラベルを式(\ref{eq:deepfool_pred})のように定義する．

\begin{align}
    y^t = \arg\max_k Z_k(x^t)
    \label{eq:deepfool_pred}
\end{align}

多クラス分類において，ラベル$k$への決定境界を考えるため，現在の予測ラベル$y^t$とラベル$k$の正規化前の出力の差を式(\ref{eq:deepfool_gk})のように定義する．

\begin{align}
    g_k(x) = Z_k(x) - Z_{y^t}(x) \quad (g_k(x)\in \mathbb{R})
    \label{eq:deepfool_gk}
\end{align}

この$g_k(x)$が正になる領域はラベル$k$として分類される領域であり，$g_k(x) = 0$となる点の集合がラベル$y^t$とラベル$k$の決定境界を表す．

点$x^t$の近傍において，$g_k(x)$を一次のテイラー展開で近似すると，式(\ref{eq:deepfool_taylor})が得られる．

\begin{align}
    g_k(x) \approx g_k(x^t) + \nabla g_k(x^t)^\top (x - x^t)
    \label{eq:deepfool_taylor}
\end{align}

この近似のもとでは，決定境界$g_k(x) = 0$は$\nabla g_k(x^t)$を法線ベクトルとする超平面として表される．

\subsection{最短摂動の導出}

線形近似された境界$g_k(x) = 0$に対して，点$x^t$からの$\ell_2$最短摂動を導出する．基本原理で述べたように，最短摂動は超平面に垂直な方向に，点と超平面の距離だけ移動することで得られる．式(\ref{eq:deepfool_taylor})の近似境界において，法線ベクトルは$\nabla g_k(x^t)$である．現在のラベルが$y^t$であるため$g_k(x^t) < 0$であることに注意すると，ラベル$k$の境界への最短摂動$r^t_k$は式(\ref{eq:deepfool_rk})で表される．

\begin{align}
    r^t_k = -\frac{g_k(x^t)}{\|\nabla g_k(x^t)\|_2^2} \nabla g_k(x^t)
    \label{eq:deepfool_rk}
\end{align}

この摂動の$\ell_2$ノルムは点$x^t$からラベル$k$の境界までの距離に等しい．

\subsection{更新式とアルゴリズム}

全てのラベル$k \neq y^t$について$\|r^t_k\|_2$を計算し，最も境界が近いラベル$k^*$を式(\ref{eq:deepfool_kstar})により選択する．

\begin{align}
    k^* = \arg\min_{k \neq y^t} \|r^t_k\|_2
    \label{eq:deepfool_kstar}
\end{align}

実際のニューラルネットワークは非線形であるため，線形近似による境界は真の境界からずれている．このずれを考慮し，境界を確実に越えるためにオーバーシュート係数$\eta > 0$（典型的には$\eta = 0.02$）を用いて，式(\ref{eq:deepfool_update})のように更新する．

\begin{align}
    x^{t+1} = x^t + (1 + \eta) r^t_{k^*}
    \label{eq:deepfool_update}
\end{align}

この更新を，予測ラベルが変化する（$k(x^{t+1}) \neq k(x^0)$）か，または最大反復回数$T$に達するまで繰り返す．最終的に得られる摂動$\delta = x^T - x^0$が，DeepFoolによる敵対的摂動となる．

DeepFoolの全体の手順をAlgorithm \ref{alg:deepfool}に示す．

\begin{algorithm}[htbp]
\caption{DeepFool}
\label{alg:deepfool}
\begin{algorithmic}[1]
\REQUIRE 入力画像$x$，分類器出力$Z$，オーバーシュート係数$\eta$，最大反復回数$T$
\ENSURE 敵対的サンプル$x_{\text{df}}$
\STATE $x^0 \leftarrow x$，$t \leftarrow 0$
\WHILE{$k(x^t) = k(x^0)$ \AND $t < T$}
    \FOR{$k \neq y^t$}
        \STATE $g_k(x^t) \leftarrow Z_k(x^t) - Z_{y^t}(x^t)$
        \STATE $r^t_k \leftarrow -\frac{g_k(x^t)}{\|\nabla g_k(x^t)\|_2^2} \nabla g_k(x^t)$
    \ENDFOR
    \STATE $k^* \leftarrow \arg\min_{k \neq y^t} \|r^t_k\|_2$
    \STATE $x^{t+1} \leftarrow x^t + (1 + \eta) r^t_{k^*}$
    \STATE $t \leftarrow t + 1$
\ENDWHILE
\RETURN $x_{\text{df}} \leftarrow x^t$
\end{algorithmic}
\end{algorithm}

\section{Madryらの論文における着目実験}
\label{sec:madry_experiment}
Madryらは\cite{PGD}において，PGD攻撃がロバスト性評価に適した手法であることを検証するため，損失の収束に関する実験を行っている．

\subsection{実験概要}
この実験では，ランダム初期化PGD攻撃を複数回実行し，各実行における反復毎の損失の推移を可視化している．PGD攻撃は制約付き非凸最適化問題を解いているため，初期点によって異なる局所最適解に到達する可能性がある．しかし，異なるランダム初期点から開始しても損失曲線が同程度の局所最大解に収束するならば，PGD攻撃は安定して同程度の攻撃強度を達成していると解釈できる．

\subsection{実験結果}
Madryらの実験では，MNISTおよびCIFAR10の自然学習モデル（通常の訓練で得られたモデル）と敵対的学習モデル（PGD攻撃による敵対的サンプルを含むデータセットで訓練したモデル）に対してこの検証が行われた．その結果，いずれのモデルにおいても複数のランダム初期点から開始した損失曲線が概ね同程度の局所最大解に収束することが示された．

本研究では，この実験を定量的に拡張するため，次章で述べる評価指標を用いて収束性を検証する．
