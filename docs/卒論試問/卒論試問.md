---
marp: true
theme: base
size: 4:3
paginate: true
---

<!-- _class: lead -->

# 画像分類モデルへの PGD 攻撃における初期化手法の検証

2026 年 2 月 16 日

九州大学 工学部 電気情報工学科

竹内研究室

執行 凱斗

---

<!--header: 卒論試問：執行凱斗-->

# 研究背景

- 近年，AI による画像分類の精度が向上し，医療分野などで応用
- しかし，

	<div style="text-align: center; margin: 1.0em 0">
		<b>人間に知覚できないノイズ（敵対的摂動）を画像に加える</b>
	</div>

	 ことで，画像分類モデルを誤分類させられる場合がある
- このような攻撃を**敵対的攻撃**，
	敵対的攻撃により作られた画像を**敵対的サンプル**と呼ぶ
	- 自動運転や医療画像診断など誤分類が許されない分野で深刻

→ 画像分類モデルが

<div style="text-align: center; margin: 1.0em 0">
<b>敵対的攻撃にどれだけ耐えられるか（敵対的ロバスト性）</b>
</div>

 <div style="padding-left: 1em; margin: 1.0em 0">を正確に調べることが重要</div>

---

# 関連研究：PGD 攻撃

![bg right:35%](pgd.png)

**PGD 攻撃**（Projected Gradient Descent attack）$^{[1]}$：モデルの敵対的ロバスト性を調べるための代表的な敵対的攻撃手法

- ここで扱うモデルは入力画像に対し各ラベルの予測確率を出力
- **損失** $L(x, y) = -\log f_y(x)$ は，正解ラベル $y$ に対するモデルの予測確率 $f_y(x)$ が低いほど大きくなる

PGD 攻撃は，
<div style="text-align: center; margin-top: -0.8em">

<b>摂動の $\ell_\infty$ ノルムが $\varepsilon$（許容する摂動の大きさ）以下</b>

</div>
<div style="padding-left: 1em; margin: 0.2em 0">という制約の下で<b>損失が大きくなる方向</b>への摂動の更新を反復</div>

> \[1]: Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks," ICLR, 2018.
> \[3]: 稲岡 陽向，研究室内スライド「L1 正則化ニューラルネットワークの特徴量混合による敵対的堅牢性」，2025 年 5 月 27 日

---

# 関連研究：Madry らの実験

**Madry らの実験**$^{[1]}$

- Madry らは（ランダム初期化）PGD 攻撃を提案し，
	
	<div style="text-align: center; margin: 1.0em 0">
	「どのランダムな初期点から始めても，十分な反復を経ると，<b>同じぐらいの強い攻撃（損失）</b>に達する」
	</div>

	ことを，2 つのデータセットで**損失の推移**を調べることで確認

**問題提起**

- 確認したのは**攻撃の強さが安定する**ことのみ
- **モデルが誤分類するか**，**何反復目で間違えるか**，**モデルや初期化手法**を変えるとその結果はどう変化するのか，などは未検証

> \[1]: Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks," ICLR, 2018.

---

# 研究の目的

- Madry らの実験 $^{[1]}$（損失の推移の可視化）を再現した上で，
	- **実際に誤分類が達成されるかどうか**
	- **何反復目で誤分類するか**

	を調べ，損失だけでなく誤分類の観点からも PGD 攻撃を評価
<br>
 - <b>異なるモデルや初期化手法</b>を用いた追加実験を行い，**誤分類に必要な反復数**への影響を評価
	  - 新たな初期化手法を提案し，より少ない反復数で誤分類を達成できるかどうかを検証

> \[1]: Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks," ICLR, 2018.

---

# 提案手法

- **PGD 攻撃**：制約範囲内で**損失を大きくする方向**への摂動の更新を反復
- **DeepFool**$^{[2]}$：モデルの予測ラベルが変わる境界（**ラベル境界**）のうち，**最も近いラベル境界への最短方向**への摂動の更新を反復する敵対的攻撃手法

DeepFool で見つけた境界付近の点を初期点にすれば PGD 攻撃の誤分類に必要な反復数を減らせるのではないか

「**DeepFool 初期化**」を提案：DeepFool が見つけた点を前述の $\ell_\infty$ 制約内に射影し，PGD の初期点として用いる
「**Multi-DeepFool 初期化**」を提案：1 方向だけでなく，全てのラベルとの境界に向かう複数の初期点を用いる

> \[2]: Moosavi-Dezfooli et al., "DeepFool," CVPR, 2016.

---

# 実験対象

  **データセット**（10 クラス）：MNIST（手書き数字）・CIFAR10（カラー自然画像）

  **モデル**：敵対的サンプルを訓練に含めると敵対的攻撃に対する堅牢性が高まるため，その割合を変えて堅牢性の異なるモデルを用意

  | モデル名 | 訓練データ |
  |---|---|
  | nat | 通常のデータのみ |
  | adv | 敵対的サンプルのみ |
  | nat_and_adv | 通常のデータ 50% + 敵対的サンプル 50% |
  | weak_adv | 弱い敵対的サンプルのみ |

  **初期化手法**：クリーン（初期点が元画像）・ランダム・DeepFool・Multi-DeepFool

---

# 実験内容

各データセットに対し，4 つのモデルと 4 つの初期化手法の全組み合わせで，100 サンプルに対して各 100 反復の PGD 攻撃を行い，各反復時点での誤分類の状況を調べた．

- 独自の指標「**誤分類達成率**」を導入：各反復時点で，100 サンプル中モデルが誤分類しているサンプルの割合

MNIST の場合の，各反復時点における誤分類達成率の推移をモデル・初期化手法毎に可視化したヒートマップを次のページに示す．

---

<!-- _class: fullimage -->
<!-- _header: '' -->
<!-- _paginate: false -->

![](mnist_misclassification_heatmap.png)

---

# まとめ

- Madry らの再現実験において，MNIST，CIFAR10 ともに損失曲線は同じように収束した一方，CIFAR10 は MNIST より**はるかに少ない反復数で誤分類**を達成した．
- 提案手法，特に **DeepFool 初期化**は**十分に堅牢なモデル**に対しては効果が限定的であったが，**それ以外のモデル**に対してはランダム初期化より少ない反復数で誤分類した．これにより，
	
	<div style="text-align: center; margin: 0.1em 0">
		<b>初期化手法の工夫で敵対的攻撃の計算時間を減らせる可能性</b>
	</div>

	<div style="margin: 0.1em 0">を示した．</div>

<div style="margin-top: -0.5em">
	<b>今後の展望</b>
</div>

- より大規模なデータセット・モデルへの適用
- 堅牢なモデルに対するより長い反復数や他の攻撃手法での検証

> \[1]: Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks," ICLR, 2018.

---

# 参考文献

1. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, "Towards Deep Learning Models Resistant to Adversarial Attacks," International Conference on Learning Representations (ICLR), 2018.
2. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard, "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks," IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
3. 稲岡陽向「スパースニューラルネットワークの混合による敵対的堅牢性の最適化」九州大学工学部卒業論文，2024 年

