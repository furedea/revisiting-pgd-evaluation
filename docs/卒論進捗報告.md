---
marp: true
theme: base
size: 4:3
paginate: true
tags:
  - 再現実験
---

<!-- _class: lead -->

# 卒論進捗報告

## 2026/1/14

---

# 卒論テーマ (中間発表の復習)

**初期化手法とイタレーションに関する実験を用いた PGD によるロバスト性評価の再検証**

- PGD によるロバスト性の評価はどのくらい信頼できるものなのか？
- どの初期化手法でどのくらいのイタレーションを回せば損失がプラトー付近になるのか？
	- より少ないイタレーションで損失をプラトー付近に到達させられるかもしれない**DeepFool の境界情報を用いた初期化**を提案
	- 各場合のイタレーション毎の損失，損失の収束までのイタレーションの平均や分布，モデルの精度の推移を調査

---

# 卒論テーマに関わる論文 (復習)

**着目した論文**

PGD をモデルの敵対的ロバスト性を学習・評価する攻撃手法として提案した論文 $^{[1]}$

**着目した実験**

PGD が安定してある程度同じ局所最大値を見つけられるか，ひいてはロバスト性評価のための攻撃手法として適切なのかを，**イタレーション毎の損失の収束の仕方**を可視化することで確かめる実験

> \[1]: Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, “Towards Deep Learning Models Resistant to Adversarial Attacks,” International Conference on Learning Representations (ICLR), 2018.

---

# 卒論テーマに関わる実験① (復習)

---

# 卒論テーマに関わる実験② (復習)

---

# 再現実験の概要

- 前ページの Figure 1, 2 の実験 (Madry et al, 2018) を再現
- モデル：論文に記載があった Madry らのリポジトリ $^{[2]}$ にある事前学習された 4 つの公式配布モデル (MNIST，CIFAR10 それぞれの自然学習モデルと PGD 敵対的学習モデル)
	- 着目した実験で使用されたモデルと完全に同一である保証は本文から確定できないため，本再現では曲線形状・収束傾向・実装条件依存性などの確認が主目的
- 実行環境：Python 3.6.9，tensorflow-gpu 1.15.5
	- 上のモデルが TF 1.15 のもので TF 2.0 以降や Pytorch に対応してなさそうだったため
	- 研究室の GPU(H100) は TF1 に互換性がなく (H100 は TF1.15 以降に台頭)，主に CPU 実行 (コア数やバッチ処理)

> \[2]: https://github.com/MadryLab/mnist_challenge, https://github.com/MadryLab/cifar10_challenge

---

# 再現実験の設定 (MNIST)

- 前ページの Figure 1, 2 の実験設定 (Madry et al, 2018) 再現
- データ：MNIST(tensorflow.examples.tutorials.mnist) のテストデータ
	- モデルが正しく分類する画像のみ採用
- 攻撃：PGD (random start)
- パラメータ： $\varepsilon = 0.3, \alpha = 0.01, \text{iterations} = 100, \text{restarts} = 20$
- 制約：$\ell_\infty$-ball へ射影，敵対的例の画素範囲をクリップ $[0,1]$

---

# 再現実験 (MNIST, Naturally_trained)①

Figure 3：MNIST の自然学習モデルでの結果
- 上図の見方は Figure 1, 2 と同様
- 真ん中の図はイタレーションのどこで誤分類したかを示す
	- 横軸がイタレーションで縦軸はリスタート数
- 下図：左が入力画像
- 下図：右が誤分類後 (誤分類していない場合は最大イタレーション) の敵対的サンプル

---

# 再現実験 (MNIST, Adv_trained)②

Figure 4：MNIST の敵対的学習モデルでの結果 (見方は Figure 1, 2 と同様)
- 左図が論文の実験
- 右図が再現実験

---

# 再現実験の設定 (CIFAR10)

- 前ページの Figure 1, 2 の実験設定 (Madry et al, 2018) 再現
- データ：CIFAR10(tf.keras.datasets.cifar10) のテストデータ
	- モデルが正しく分類する画像のみ採用
- 攻撃：PGD (random start)
- パラメータ： $\varepsilon = \frac {8} {255}, \alpha = \frac {2} {255}, \text{iterations} = 100, \text{restarts} = 20$
- 制約：$\ell_\infty$-ball へ射影，敵対的例の画素範囲をクリップ $[0,1]$

---

# 再現実験 (CIFAR10, Naturally_trained)①

Figure 5: CIFAR10 の自然学習モデルでの結果 (見方は Figure 1, 2 と同様)
- 左図が論文の実験
- 右図が再現実験

---

# 再現実験 (CIFAR10, Adv_trained)②

Figure 6：CIFAR10 の敵対的学習モデルでの結果 (見方は Figure 1, 2 と同様)
- 左図が論文の実験
- 右図が再現実験

---

# 初期化手法の提案 (復習)

**DeepFool の境界情報を用いた初期化手法**

- DeepFool で導出した敵対的サンプルから PGD を開始
	- DeepFool：現在の点から**最も近いクラス境界へ最短で向かう方向**に摂動を更新
	- PGD：入力に関する**損失の勾配方向**に摂動を更新
- DeepFool は $\ell_2$ **最短摂動**を狙う攻撃のため，作った敵対的サンプルを $\ell_\infty$ 制約範囲に射影して初期点とする
- DeepFool は初期点が固定されておりランダム性がないため，初期点に微小なランダムノイズを入れることを検討
	- DeepFool の敵対的サンプルを作った後各ピクセルにランダムノイズ ($\text{jitter} = 0.001$ の場合 $[-0.001, 0.001]$ の一様分布から) を入れる
- パラメータ：$T = 50, \eta = 0.02, \text{jitter} = 0.0$

---

# 追加実験 (DeepFool-init, MNIST, Naturally_trained)

Figure 7：MNIST の自然学習モデルでの結果 (見方は Figure 1, 2 と同様)
- 左図が再現実験
- 右図が追加実験

---

# 追加実験 (DeepFool-init, MNIST, Adv_trained)

Figure 8：MNIST の敵対的学習モデルでの結果 (見方は Figure 1, 2 と同様)
- 左図が再現実験
- 右図が追加実験

---

# 追加実験 (DeepFool-init, CIFAR10, Naturally_trained)

Figure 9：CIFAR10 の自然学習モデルでの結果 (見方は Figure 1, 2 と同様)
- 左図が再現実験
- 右図が追加実験

---

# 追加実験 (DeepFool-init, CIFAR10, Adv_trained)

Figure 10：CIFAR10 の敵対的学習モデルでの結果 (見方は Figure 1, 2 と同様)
- 左図が再現実験
- 右図が追加実験

---

# Deepfool-init の問題点

- 追加実験を見る限り，Deepfool-init による損失は現状の実験設定だとほぼ意味がない
	- クロスエントロピー損失を採用しているため，Deepfool により敵対的サンプルが異なるラベルの境界に近づいても，損失自体は 0.693 程度になる
		- 自然学習モデルの場合，PGD 攻撃により損失が極端に大きくなるため，Deepfool による損失が誤差になりやすい
		- 敵対的学習モデルの場合，PGD 攻撃により損失はあまり大きくならず，Deepfool による損失が効果的に働く
			- しかし Deepfool の敵対的サンプルへの PGD の $\ell_\infty$ 制約の適用により，損失が大きく減る場合がある

→ Deepfool の敵対的サンプルへの PGD の $\ell_\infty$ 制約の適用につい
　て考え直す

---

# Deepfool-init における $\ell_\infty$ 制約の適用

Deepfool による敵対的サンプル を $\ell_\infty$-ball に射影

**従来**：成分毎にクリップ (i 番目の成分の場合)

$$\{\boldsymbol{x}_{init}\}_i = \text{clip}(\{\boldsymbol{x}_{df}\}_i, \{\boldsymbol{x}_{nat}\}_i - \varepsilon, \{\boldsymbol{x}_{nat}\}_i + \varepsilon)$$

- $\boldsymbol{x}_{nat}$：入力データ
- $\boldsymbol{x}_{df}$：Deepfool により作られた敵対的サンプル
- $\boldsymbol{x}_{init}$：$\boldsymbol{x}_{df}$ を $\ell_\infty$-ball に射影した PGD の入力開始点
- $\varepsilon$：$\|\boldsymbol{x}_{init} - \boldsymbol{x}_{nat}\|_\infty \le \varepsilon$ を満たす

---

# Deepfool-init の改良案 (スケーリング)

**提案**：摂動のスケーリング (方向は保って半径だけ合わせる)

Deepfool により作られた摂動を $\boldsymbol{\delta}_{df} = \boldsymbol{x}_{df} - \boldsymbol{x}_{nat}$ とすると

$$\boldsymbol{x}_{init} = \boldsymbol{x}_{nat} + s \boldsymbol{\delta}_{df} \quad (s = \min(1, \frac {\varepsilon} {\|\boldsymbol{\delta}_{df}\|_\infty}))$$

- $\|\boldsymbol{x}_{init} - \boldsymbol{x}_{nat}\|_\infty \le \varepsilon$ を満たす，かつ方向 (比率) は保存
- 結論から言うと clip の方が良かった

---

# 追加実験 (DeepFool-init, MNIST, Adv_trained，Scale)

---

# 追加実験 (DeepFool-init, MNIST, Naturally_trained，Maxloss)

**提案**：Deepfool の (clip した) 各点で最も大きい損失の点を採用

---

# 追加実験 (DeepFool-init, MNIST, Adv_trained，Maxloss)

---

# 追加実験 (DeepFool-init, CIFAR10, Naturally_trained，Maxloss)

---

# 追加実験 (DeepFool-init, CIFAR10, Adv_trained，Maxloss)

---

# 本実験の方針

- 初期化 3 種（input / random / DeepFool）× iterations を系統比較
- 指標：loss 曲線 + プラトー到達反復数分布 + robust accuracy
- 調査対象：
	- 各モデル・初期化に対するイタレーション毎の損失の推移
	- 損失の収束までのイタレーションの平均や分布
	- モデルの精度の推移

---

# 参考文献

1. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, “Towards Deep Learning Models Resistant to Adversarial Attacks,” International Conference on Learning Representations (ICLR), 2018.
2. Moosavi-Dezfooli, S.-M., Fawzi, A., & Frossard, P. (2016). _DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks_. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2016), 2574-2582. IEEE.
3. https://github.com/MadryLab/mnist_challenge
4. https://github.com/MadryLab/cifar10_challenge
